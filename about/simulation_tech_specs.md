# Simulation Engine Technical Implementation

This document describes the technical architecture of the `simulation` node and its integration with the `recruiter` module.

## 1. Persona Sourcing Strategy

The system uses a hybrid approach to source test subjects for interviews, ensuring both speed and depth.

### A. The "Recruiter" (Vector Search)
*   **Source**: A local FAISS/Chroma vector index containing ~20,000 real-world user personas (from `personas_index.json`).
*   **Logic**:
    1.  The `Generator` produces a startup concept.
    2.  `GoogleRecruiter` embeds the concept title + description using `text-embedding-004`.
    3.  Performs a K-Nearest Neighbors (KNN) search to find the top 10 most semantically relevant personas.
    4.  **Selection**: The system selects the top `N` candidates (default 3-5) closest to the concept.
*   **Data Structure**: Returns `RichPersona` objects containing:
    *   `original_text`: The full raw biography.
    *   `attitude`: An inferred attribute (e.g., "Skeptical", "Early Adopter").

### B. Fallback (Synthetic)
*   If the local vector search fails or returns low-confidence matches, the system falls back to the `Researcher` node's output.
*   **Source**: `InterviewGuide` generated by the LLM.
*   **Data Structure**: Synthetic `TargetPersona` objects (Name, Role, Archetype, Context).

## 2. The Simulation Loop (`simulation_node`)

The core loop orchestrates the interaction between the system (Interviewer) and the simulated user (LLM).

### Data Mapping
The system normalizes all inputs into a compatible `TargetPersona` structure before the loop:
```python
class TargetPersona(BaseModel):
    name: str       # e.g., "Elena"
    role: str       # e.g., "Small Business Owner"
    archetype: str  # e.g., "Pragmatist"
    context: str    # e.g., "Owning a cafe for 5 years..."
```
*   **RichPersona (Vector)**: `context` is enriched with the full biographical `original_text` to give the LLM deep backstory.
*   **Synthetic Persona**: `context` is the paragraph generated by the Researcher.

### Execution Flow
For each selected persona:
1.  **Prompt Engineering**: A system prompt is constructed to enforce role-play.
    *   *Constraint*: "You are {Name}, a {Role}. Your personality is {Archetype}."
    *   *Context Injection*: "Your backstory: {Context}."
    *   *Instruction*: "Answer the interviewer's questions honestly based *only* on your context. Do not be overly polite."
2.  **Stateless Execution**: The entire interview (questions + context) is sent in a single LLM call to `gemini-1.5-flash` or `gpt-4o` (depending on configuration).
    *   *Why?* The interview guide is static. We don't need a multi-turn chat for this MVP; we need the user's reaction to the specific "Mom Test" questions designed by the Researcher.
3.  **Structured Output**: The LLM is forced to return a JSON object (`InterviewResult`):
    *   `transcript_summary`: A synthesized summary of their answers.
    *   `pain_level` (1-10): How acute the problem is for them.
    *   `willingness_to_pay` (1-5): Likelihood of purchase.

## 3. Artifact Persistence

After all interviews are completed, the results are aggregated and saved:
*   **File**: `experiments/<Idea_Title>/interviews_transcript.md`
*   **Format**: Human-readable Markdown containing profiles, quantitative scores, and the transcript summary for each participant.

## 4. Error Handling
*   **Mock Mode**: A `MOCK_SIMULATION` flag allows developers to bypass real LLM calls for rapid UI testing, returning hardcoded dummy data.
*   **JSON Repair**: A utility `extract_json_from_text` handles cases where the LLM wraps the JSON in Markdown code blocks or adds conversational filler.
