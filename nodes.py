import json
import re
import pathlib
from langchain_core.messages import HumanMessage, SystemMessage
from config import (
    llm_generator, llm_critic, 
    GENERATOR_SYSTEM_PROMPT, CRITIC_SYSTEM_PROMPT, 
    RESEARCHER_SYSTEM_PROMPT, SIMULATION_SYSTEM_PROMPT, 
    ANALYST_SYSTEM_PROMPT, MOCK_SIMULATION
)
from models import (
    BusinessIdea, CritiqueFeedback, InterviewGuide, 
    InterviewResult, UserPersona, ResearchReport
)
from state import GraphState
from utils import extract_json_from_text

def generator_node(state: GraphState) -> GraphState:
    print(f"\n--- GENERATOR NODE (Iteration {state['iteration_count']}) ---")
    
    # Ð’ÐÐ–ÐÐž: ÐœÑ‹ ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ .with_structured_output, Ñ‚Ð°Ðº ÐºÐ°Ðº Ð¾Ð½ Ð½ÐµÑÑ‚Ð°Ð±Ð¸Ð»ÐµÐ½
    # ÐœÑ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ invoke Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð¼ Ñ€ÑƒÐºÐ°Ð¼Ð¸.
    
    # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ JSON-ÑÑ…ÐµÐ¼Ñƒ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð½Ð°Ð»Ð° Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
    schema_instruction = """
    ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: Ð¢Ð²Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¡Ð¢Ð ÐžÐ“Ðž Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¼ JSON Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð¼.
    ÐÐ•Ð¢ Markdown Ð±Ð»Ð¾ÐºÐ¾Ð² ```json
    ÐÐ•Ð¢ Ð²Ð²Ð¾Ð´Ð½Ñ‹Ñ… ÑÐ»Ð¾Ð²
    ÐÐ•Ð¢ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ñ… Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ñ‚Ð¸Ð¿Ð° {"BusinessIdea": {...}}
    
    Ð¢ÐžÐ§ÐÐ«Ð™ Ð¤ÐžÐ ÐœÐÐ¢ (ÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¾Ð´Ð¸Ð½-Ð²-Ð¾Ð´Ð¸Ð½, Ð¼ÐµÐ½ÑÑ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð—ÐÐÐ§Ð•ÐÐ˜Ð¯):
    {
      "title": "ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚Ð°Ñ€Ñ‚Ð°Ð¿Ð° Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼",
      "description": "ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð° Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼",
      "monetization_strategy": "ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼",
      "target_audience": "Ð¦ÐµÐ»ÐµÐ²Ð°Ñ Ð°ÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼"
    }
    
    ÐŸÐ Ð˜ÐœÐ•Ð  ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐžÐ“Ðž ÐžÐ¢Ð’Ð•Ð¢Ð:
    {
      "title": "Ð—Ð°Ð±Ð¾Ñ‚Ð°.Ð Ð¤",
      "description": "ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° Ð´Ð»Ñ ÑƒÑ…Ð¾Ð´Ð° Ð·Ð° Ð¿Ð¾Ð¶Ð¸Ð»Ñ‹Ð¼Ð¸ Ð»ÑŽÐ´ÑŒÐ¼Ð¸",
      "monetization_strategy": "ÐŸÐ¾Ð´Ð¿Ð¸ÑÐºÐ° 3000â‚½/Ð¼ÐµÑ + ÐºÐ¾Ð¼Ð¸ÑÑÐ¸Ñ 15% Ð·Ð° ÑƒÑÐ»ÑƒÐ³Ð¸",
      "target_audience": "Ð Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‰Ð¸Ðµ Ð²Ð·Ñ€Ð¾ÑÐ»Ñ‹Ðµ Ð´ÐµÑ‚Ð¸ (35-55 Ð»ÐµÑ‚) Ð² Ð³Ð¾Ñ€Ð¾Ð´Ð°Ñ…-Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¸ÐºÐ°Ñ…"
    }
    
    Ð—ÐÐŸÐ Ð•Ð©Ð•ÐÐž:
    - Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€ÑƒÑÑÐºÐ¸Ðµ Ð¸Ð¼ÐµÐ½Ð° ÐºÐ»ÑŽÑ‡ÐµÐ¹ (ÐÐ°Ð·Ð²Ð°Ð½Ð¸ÐµÐŸÑ€Ð¾Ð´ÑƒÐºÑ‚Ð°, ÐšÑ€Ð°Ñ‚ÐºÐ¾ÐµÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ)
    - Ð¡Ð¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹
    - Ð”Ð¾Ð±Ð°Ð²Ð»ÑÑ‚ÑŒ ```json Ð¸Ð»Ð¸ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ markdown
    
    ÐÐÐ§Ð˜ÐÐÐ™ ÐžÐ¢Ð’Ð•Ð¢ Ð¡Ð ÐÐ—Ð£ Ð¡ { Ð¸ Ð—ÐÐšÐÐÐ§Ð˜Ð’ÐÐ™ }
    """
    
    if state["iteration_count"] == 0:
        print(">> Generating initial ZERO-TO-ONE concept...")
        user_content = f"""
        USER INPUT: '{state['initial_input']}'
        
        Task: Synthesize a Unicorn startup concept for the RUSSIAN MARKET (2025).
        """
    elif state.get("research_report") and not state.get("critique"):
        print(">> PIVOTING based on USER RESEARCH...")
        current_json = state["current_idea"].model_dump_json(indent=2)
        report_json = state["research_report"].model_dump_json(indent=2)
        
        user_content = f"""
        USER RESEARCH COMPLETED. UPDATE THE IDEA.
        
        PREVIOUS IDEA:
        {current_json}
        
        RESEARCH FINDINGS:
        {report_json}
        
        INSTRUCTIONS:
        1. Discard features that users rejected (see 'rejected_hypotheses').
        2. Double down on 'confirmed_hypotheses'.
        3. Implement the 'pivot_recommendation'.
        
        {schema_instruction}
        """
    elif state.get("critique"):
        print(">> PIVOTING based on Critique...")
        current_json = state["current_idea"].model_dump_json(indent=2)
        critique_json = state["critique"].model_dump_json(indent=2)
        
        user_content = f"""
        CRITIQUE RECEIVED. YOU MUST ITERATE OR PIVOT.
        
        PREVIOUS IDEA:
        {current_json}
        
        CRITIC FEEDBACK:
        {critique_json}
        
        INSTRUCTIONS:
        1. Address the fatal flaws.
        2. Focus on Russian local tech (VK, Telegram, SPB, Gosuslugi).
        
        {schema_instruction}
        """
    else:
        # Fallback - should not happen in normal flow, but prevents crashes
        print(">> WARNING: Unexpected state, generating fallback...")
        user_content = f"""
        USER INPUT: '{state['initial_input']}'
        
        Task: Refine the existing idea.
        {schema_instruction}
        """

    messages = [
        SystemMessage(content=GENERATOR_SYSTEM_PROMPT),
        HumanMessage(content=user_content)
    ]

    # --- RETRY & PARSE LOGIC ---
    new_idea = None
    last_error = None
    
    for attempt in range(3):
        try:
            print(f"   -> Invoking LLM (Attempt {attempt + 1})...")
            
            response = llm_generator.invoke(messages)
            raw_content = response.content
            
            # --- FIX: ÐžÐ‘Ð ÐÐ‘ÐžÐ¢ÐšÐ Ð¡ÐŸÐ˜Ð¡ÐšÐ ---
            # Ð•ÑÐ»Ð¸ Gemini Ð²ÐµÑ€Ð½ÑƒÐ» ÑÐ¿Ð¸ÑÐ¾Ðº Ð±Ð»Ð¾ÐºÐ¾Ð² [{'type': 'text', 'text': '...'}]
            if isinstance(raw_content, list):
                print(f"   -> Detected list output, converting to string...")
                raw_content = "".join([
                    block.get("text", "") 
                    for block in raw_content 
                    if isinstance(block, dict) and "text" in block
                ])
            # -----------------------------
            
            # Ð§Ð¸ÑÑ‚Ð¸Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚
            cleaned_json_str = extract_json_from_text(raw_content)
            
            # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð² Python dict, Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ð² Pydantic
            data_dict = json.loads(cleaned_json_str)
            new_idea = BusinessIdea(**data_dict)
            
            break # Ð£ÑÐ¿ÐµÑ…!
            
        except Exception as e:
            print(f"   -> Parse Error on attempt {attempt + 1}: {e}")
            print(f"   -> Raw Output causing error: {raw_content[:500]}...") 
            last_error = e

    if new_idea is None:
        print("   -> CRITICAL ERROR: Failed to parse JSON from Generator.")
        # Ð§Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ ÐºÑ€Ð°ÑˆÐ¸Ñ‚ÑŒ Ð²ÐµÑÑŒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ, Ð²ÐµÑ€Ð½ÐµÐ¼ Ð·Ð°Ð³Ð»ÑƒÑˆÐºÑƒ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¾Ð¹
        new_idea = BusinessIdea(
            title="ÐžÑˆÐ¸Ð±ÐºÐ° Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ (Parsing)",
            description=f"ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð²ÐµÑ€Ð½ÑƒÐ»Ð° Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON. ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {str(last_error)}",
            monetization_strategy="N/A",
            target_audience="N/A"
        )

    print(f"   -> Generated: {new_idea.title}")

    # If we are iterating (critique exists), we should clear the previous research report and critique
    # to allow for a fresh research cycle if enabled.
    return {
        "current_idea": new_idea,
        "iteration_count": state["iteration_count"] + 1,
        "research_report": None, # Clear for next cycle
        "critique": None         # Clear for next cycle
    }

def critic_node(state: GraphState) -> GraphState:
    """
    Simulates and critiques the idea using ChatGPT 5.1 (Reasoning).
    """
    print("\n--- CRITIC NODE ---")
    
    # 1. Bind Structured Output
    structured_llm = llm_critic.with_structured_output(CritiqueFeedback)
    
    current_idea = state["current_idea"]
    
    # 2. Construct Data-Only User Message
    # The System Prompt already tells GPT-5.1 to "Simulate", so we just provide the data.
    user_content = f"""
    CANDIDATE STARTUP IDEA FOR EVALUATION:
    
    {current_idea.model_dump_json(indent=2)}
    
    Run your simulation and provide the verdict.
    """
    
    # 3. Invoke LLM with the Reasoning System Prompt
    messages = [
        SystemMessage(content=CRITIC_SYSTEM_PROMPT),
        HumanMessage(content=user_content)
    ]
    
    try:
        feedback = structured_llm.invoke(messages)
        
        # Validate the response
        if feedback is None:
            raise ValueError("LLM returned None. Check API key and model availability.")
        
        print(f"   -> Verdict: {feedback.is_approved} (Score: {feedback.score}/10)")
        print(f"   -> Key Feedback: {feedback.feedback[:100]}...") # Print preview
    except Exception as e:
        print(f"   -> ERROR in critic_node: {e}")
        # Return a default critique to prevent crash
        feedback = CritiqueFeedback(
            is_approved=False,
            feedback=f"Critique failed due to LLM error: {str(e)}",
            score=1
        )
        
    return {"critique": feedback}

def researcher_node(state: GraphState) -> GraphState:
    """
    Generates hypotheses and an interview guide using Gemini 3 Pro.
    Saves the guide to a local file.
    """
    print(f"\n--- RESEARCHER NODE ---")
    
    current_idea = state["current_idea"]
    
    # 1. Construct User Message
    schema_instruction = """
    ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž Ð”Ð›Ð¯ JSON:
    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ðµ Ð¸Ð¼ÐµÐ½Ð° Ð¿Ð¾Ð»ÐµÐ¹: target_personas, questions, hypotheses_to_test
    
    ÐŸÐ Ð˜ÐœÐ•Ð  ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐžÐ“Ðž ÐžÐ¢Ð’Ð•Ð¢Ð:
    {
      "target_personas": [
        {
          "name": "Ð¢Ð°Ñ‚ÑŒÑÐ½Ð° Ð˜Ð²Ð°Ð½Ð¾Ð²Ð½Ð°",
          "role": "Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð±ÑƒÑ…Ð³Ð°Ð»Ñ‚ÐµÑ€",
          "archetype": "ÐšÐ¾Ð½ÑÐµÑ€Ð²Ð°Ñ‚Ð¾Ñ€",
          "context": "Ð Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² 1Ð¡ 15 Ð»ÐµÑ‚, Ð½ÐµÐ½Ð°Ð²Ð¸Ð´Ð¸Ñ‚ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ, Ð·Ð¿ 80Ðº"
        },
        {
          "name": "ÐÐ»ÐµÐºÑÐµÐ¹",
          "role": "ÐŸÑ€Ð¾Ð´Ð°ÐºÑ‚-Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€",
          "archetype": "ÐÐ¾Ð²Ð°Ñ‚Ð¾Ñ€",
          "context": "Ð›ÑŽÐ±Ð¸Ñ‚ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ, Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² ÑÑ‚Ð°Ñ€Ñ‚Ð°Ð¿Ðµ"
        },
        {
          "name": "Ð•Ð»ÐµÐ½Ð°",
          "role": "Ð ÑƒÐºÐ¾Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ Ð¾Ñ‚Ð´ÐµÐ»Ð°",
          "archetype": "Ð¥ÐµÐ¹Ñ‚ÐµÑ€",
          "context": "ÐÐµ Ð´Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¼ Ñ€ÐµÑˆÐµÐ½Ð¸ÑÐ¼, Ð¸Ñ‰ÐµÑ‚ Ð¿Ð¾Ð´Ð²Ð¾Ñ…"
        }
      ],
      "questions": ["Ð’Ð¾Ð¿Ñ€Ð¾Ñ 1", "Ð’Ð¾Ð¿Ñ€Ð¾Ñ 2"],
      "hypotheses_to_test": [
        {"description": "Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 1", "type": "Problem"},
        {"description": "Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 2", "type": "Solution"},
        {"description": "Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 3", "type": "Monetization"}
      ]
    }
    
    Ð’ÐÐ–ÐÐž: type Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¢ÐžÐ›Ð¬ÐšÐž "Problem", "Solution" Ð¸Ð»Ð¸ "Monetization" (ÐÐ• "Willingness to Pay"!)
    target_personas Ð´Ð¾Ð»Ð¶ÐµÐ½ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ÑŒ Ð¢ÐžÐ§ÐÐž 3 Ð¾Ð±ÑŠÐµÐºÑ‚Ð° Ñ Ð¿Ð¾Ð»ÑÐ¼Ð¸: role, archetype, context, name (Ð²ÑÐµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼)
    ÐÐµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ€ÑƒÑÑÐºÐ¸Ðµ ÐºÐ»ÑŽÑ‡Ð¸ Ð² JSON
    """
    
    user_content = f"""
    ANALYZE THIS BUSINESS IDEA AND PREPARE USER RESEARCH:
    
    {current_idea.model_dump_json(indent=2)}
    
    Task: Create a 'Mom Test' interview guide to validate this idea in the Russian market.
    
    {schema_instruction}
    """
    
    messages = [
        SystemMessage(content=RESEARCHER_SYSTEM_PROMPT),
        HumanMessage(content=user_content)
    ]
    
    # 2. Invoke LLM
    # We use the same manual parsing logic as generator_node for stability with Gemini
    interview_guide = None
    last_error = None
    
    for attempt in range(3):
        try:
            print(f"   -> Invoking Researcher (Attempt {attempt + 1})...")
            response = llm_generator.invoke(messages)
            raw_content = response.content
            
            if isinstance(raw_content, list):
                raw_content = "".join([b.get("text", "") for b in raw_content if isinstance(b, dict)])
                
            cleaned_json_str = extract_json_from_text(raw_content)
            data_dict = json.loads(cleaned_json_str)
            interview_guide = InterviewGuide(**data_dict)
            break
        except Exception as e:
            print(f"   -> Researcher Parse Error: {e}")
            last_error = e
            
    if interview_guide is None:
        print("   -> CRITICAL: Researcher failed to generate guide.")
        # Return empty/default to avoid crash
        return {"interview_guide": None, "iteration_count": state["iteration_count"]}

    print(f"   -> Generated Guide with {len(interview_guide.target_personas)} personas and {len(interview_guide.hypotheses_to_test)} hypotheses")

    # 3. File Persistence
    try:
        # Sanitize title for folder name
        safe_title = re.sub(r'[<>:"/\\|?*]', '', current_idea.title).strip().replace(' ', '_')
        # Limit length just in case
        safe_title = safe_title[:50]
        
        base_dir = pathlib.Path("experiments")
        experiment_dir = base_dir / safe_title
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        file_path = experiment_dir / "interview_guide.md"
        
        # Format Markdown with new structure
        md_content = f"# Interview Guide: {current_idea.title}\n\n"
        
        md_content += "## Target Personas\n\n"
        for i, persona in enumerate(interview_guide.target_personas, 1):
            md_content += f"### Persona {i}: {persona.name}\n"
            md_content += f"- **Role:** {persona.role}\n"
            md_content += f"- **Archetype:** {persona.archetype}\n"
            md_content += f"- **Context:** {persona.context}\n\n"
        
        md_content += "## Hypotheses to Test\n"
        for h in interview_guide.hypotheses_to_test:
            md_content += f"- **[{h.type}]** {h.description}\n"
            
        md_content += "\n## Questions (The Mom Test)\n"
        for i, q in enumerate(interview_guide.questions, 1):
            md_content += f"{i}. {q}\n"
            
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(md_content)
            
        print(f"   -> Saved guide to: {file_path}")
        
    except Exception as e:
        print(f"   -> File Save Error: {e}")

    return {
        "interview_guide": interview_guide,
        "iteration_count": state["iteration_count"]
    }

def simulation_node(state: GraphState) -> GraphState:
    """
    Simulates 3 distinct user interviews based on the guide.
    """
    print(f"\n--- SIMULATION NODE ---")
    
    interview_guide = state.get("interview_guide")
    if not interview_guide:
        print("   -> CRITICAL: No interview guide found.")
        return state
        
    current_idea = state["current_idea"]
    
    # Get personas from interview guide (generated by researcher)
    personas = interview_guide.target_personas
    if not personas or len(personas) == 0:
        print("   -> CRITICAL: No personas in interview guide.")
        return state
    
    print(f"   -> Found {len(personas)} personas to interview")
    
    raw_interviews = []
    
    # Prepare File for Transcript
    try:
        safe_title = re.sub(r'[<>:"/\\|?*]', '', current_idea.title).strip().replace(' ', '_')[:50]
        base_dir = pathlib.Path("experiments")
        experiment_dir = base_dir / safe_title
        experiment_dir.mkdir(parents=True, exist_ok=True)
        transcript_path = experiment_dir / "interviews_transcript.md"
        
        # Clear previous file
        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(f"# User Interviews: {current_idea.title}\n\n")
            
    except Exception as e:
        print(f"   -> File Setup Error: {e}")
        transcript_path = None

    # Loop & Simulate
    for p in personas:
        print(f"   -> Simulating interview with {p.name}...")
        
        # MOCK MODE CHECK
        if MOCK_SIMULATION:
            print(f"   -> [MOCK MODE] Skipping real LLM call for {p.name}")
            result = InterviewResult(
                persona=UserPersona(name=p.name, role=p.role, background=f"{p.archetype}: {p.context}"),
                transcript_summary=f"ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ {p.name} Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚, Ñ‡Ñ‚Ð¾ Ð¸Ð´ÐµÑ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð°Ñ, Ð½Ð¾ Ð´Ð¾Ñ€Ð¾Ð³Ð¾. Ð¥Ð¾Ñ‡ÐµÑ‚ Telegram-Ð±Ð¾Ñ‚ Ð²Ð¼ÐµÑÑ‚Ð¾ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ.",
                pain_level=7,
                willingness_to_pay=4
            )
            raw_interviews.append(result)
            
            # Still write to file
            if transcript_path:
                with open(transcript_path, "a", encoding="utf-8") as f:
                    f.write(f"## Interview with {result.persona.name}\n")
                    f.write(f"**Role:** {result.persona.role}\n")
                    f.write(f"**Pain Level:** {result.pain_level}/10\n")
                    f.write(f"**Willingness to Pay:** {result.willingness_to_pay}/10\n\n")
                    f.write(f"### Transcript Summary\n{result.transcript_summary}\n\n")
                    f.write("---\n\n")
            continue
        
        # REAL LLM MODE
        user_content = f"""
        Ð¢Ð« Ð˜Ð“Ð ÐÐ•Ð¨Ð¬ Ð ÐžÐ›Ð¬:
        Ð˜Ð¼Ñ: {p.name}
        ÐŸÑ€Ð¾Ñ„ÐµÑÑÐ¸Ñ: {p.role}
        ÐŸÑÐ¸Ñ…Ð¾Ñ‚Ð¸Ð¿: {p.archetype}
        ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚: {p.context}
        
        Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° â€” Ð¿Ñ€Ð¾Ð¹Ñ‚Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð²ÑŒÑŽ Ð¿Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñƒ.
        Ð‘ÑƒÐ´ÑŒ {p.archetype}. ÐžÐ¿Ð¸Ñ€Ð°Ð¹ÑÑ Ð½Ð° ÑÐ²Ð¾Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚: {p.context}
        ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ñ‡ÐµÑÑ‚Ð½Ð¾. Ð•ÑÐ»Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ñ‚ÐµÐ±Ðµ Ð½Ðµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚ â€” ÑÐºÐ°Ð¶Ð¸ Ð¾Ð± ÑÑ‚Ð¾Ð¼ Ð¿Ñ€ÑÐ¼Ð¾.
        ÐÐµ Ð¿Ñ‹Ñ‚Ð°Ð¹ÑÑ ÑƒÐ³Ð¾Ð´Ð¸Ñ‚ÑŒ Ð¸Ð½Ñ‚ÐµÑ€Ð²ÑŒÑŽÐµÑ€Ñƒ.
        
        Ð’ÐžÐŸÐ ÐžÐ¡Ð« Ð˜ÐÐ¢Ð•Ð Ð’Ð¬Ð®Ð•Ð Ð:
        {json.dumps(interview_guide.questions, ensure_ascii=False, indent=2)}
        
        Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¸ meta-analysis JSON.
        """
        
        messages = [
            SystemMessage(content=SIMULATION_SYSTEM_PROMPT),
            HumanMessage(content=user_content)
        ]
        
        # Invoke LLM (GPT-5.1 / Critic Model)
        try:
            response = llm_critic.invoke(messages)
            raw_content = response.content
            
            cleaned_json_str = extract_json_from_text(raw_content)
            data_dict = json.loads(cleaned_json_str)
            
            # Inject Name/Role into the result manually since LLM might generate random ones
            if "persona" not in data_dict:
                data_dict["persona"] = {}
            data_dict["persona"]["name"] = p.name
            data_dict["persona"]["role"] = p.role
            if "background" not in data_dict["persona"]:
                 data_dict["persona"]["background"] = f"{p.archetype}: {p.context}"

            result = InterviewResult(**data_dict)
            raw_interviews.append(result)
            
            # Append to Transcript File
            if transcript_path:
                with open(transcript_path, "a", encoding="utf-8") as f:
                    f.write(f"## Interview with {result.persona.name}\n")
                    f.write(f"**Role:** {result.persona.role}\n")
                    f.write(f"**Pain Level:** {result.pain_level}/10\n")
                    f.write(f"**Willingness to Pay:** {result.willingness_to_pay}\n\n")
                    f.write(f"### Transcript Summary\n{result.transcript_summary}\n\n")
                    # Note: We don't have the full raw dialogue text in the JSON model currently, 
                    # only the summary. The prompt asks for "Transcript" in the text generation 
                    # but the JSON schema only captures summary. 
                    # Ideally, we should capture the full text too, but for now we follow the schema.
                    f.write("---\n\n")
                    
        except Exception as e:
            print(f"   -> Simulation Error for {p.name}: {e}")
            
    return {
        "raw_interviews": raw_interviews,
        "iteration_count": state["iteration_count"]
    }

def analyst_node(state: GraphState) -> GraphState:
    """
    Analyzes interview transcripts and generates a research report.
    """
    print(f"\n--- ANALYST NODE ---")
    
    raw_interviews = state.get("raw_interviews", [])
    if not raw_interviews:
        print("   -> CRITICAL: No interviews found.")
        return state
        
    current_idea = state["current_idea"]
    
    # 1. Aggregate Context
    transcripts_text = ""
    for i, interview in enumerate(raw_interviews, 1):
        transcripts_text += f"INTERVIEW {i} ({interview.persona.role}):\n"
        transcripts_text += f"Summary: {interview.transcript_summary}\n"
        transcripts_text += f"Pain Level: {interview.pain_level}/10\n"
        transcripts_text += f"Willingness to Pay: {interview.willingness_to_pay}/10\n\n"
        
    user_content = f"""
    ANALYZE THESE INTERVIEWS:
    
    {transcripts_text}
    
    Task: Validate hypotheses and recommend a pivot.
    
    ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: Ð¢Ð²Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¡Ð¢Ð ÐžÐ“Ðž Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¼ JSON Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð¼.
    ÐÐ•Ð¢ Markdown Ð±Ð»Ð¾ÐºÐ¾Ð² ```json
    ÐÐ•Ð¢ Ð²Ð²Ð¾Ð´Ð½Ñ‹Ñ… ÑÐ»Ð¾Ð²
    
    Ð¢ÐžÐ§ÐÐ«Ð™ Ð¤ÐžÐ ÐœÐÐ¢ (ÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¾Ð´Ð¸Ð½-Ð²-Ð¾Ð´Ð¸Ð½):
    {{
      "key_insights": ["Ð˜Ð½ÑÐ°Ð¹Ñ‚ 1 Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹", "Ð˜Ð½ÑÐ°Ð¹Ñ‚ 2 Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹"],
      "confirmed_hypotheses": ["Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 1", "Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 2"],
      "rejected_hypotheses": ["Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 3"],
      "pivot_recommendation": "Ð§ÐµÑ‚ÐºÐ°Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹"
    }}
    
    Ð—ÐÐŸÐ Ð•Ð©Ð•ÐÐž:
    - Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹ Ñ‚Ð¸Ð¿Ð° {{"insight": "Ñ‚ÐµÐºÑÑ‚"}}
    - key_insights Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¼Ð°ÑÑÐ¸Ð²Ð¾Ð¼ Ð¡Ð¢Ð ÐžÐš, Ð½Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð²
    - Ð’ÑÐµ Ð¿Ð¾Ð»Ñ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹
    
    ÐÐÐ§Ð˜ÐÐÐ™ ÐžÐ¢Ð’Ð•Ð¢ Ð¡Ð ÐÐ—Ð£ Ð¡ {{ Ð¸ Ð—ÐÐšÐÐÐ§Ð˜Ð’ÐÐ™ }}
    """
    
    messages = [
        SystemMessage(content=ANALYST_SYSTEM_PROMPT),
        HumanMessage(content=user_content)
    ]
    
    # 2. Invoke LLM (Gemini 3 Pro)
    research_report = None
    try:
        response = llm_generator.invoke(messages)
        raw_content = response.content
        
        if isinstance(raw_content, list):
            raw_content = "".join([b.get("text", "") for b in raw_content if isinstance(b, dict)])
            
        cleaned_json_str = extract_json_from_text(raw_content)
        data_dict = json.loads(cleaned_json_str)
        research_report = ResearchReport(**data_dict)
        
        print(f"   -> Pivot Recommendation: {research_report.pivot_recommendation[:100]}...")
        
        # 3. File Persistence
        safe_title = re.sub(r'[<>:"/\\|?*]', '', current_idea.title).strip().replace(' ', '_')[:50]
        base_dir = pathlib.Path("experiments")
        experiment_dir = base_dir / safe_title
        experiment_dir.mkdir(parents=True, exist_ok=True)
        report_path = experiment_dir / "research_report.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"# Research Report: {current_idea.title}\n\n")
            f.write("## âœ… Confirmed Hypotheses\n")
            for h in research_report.confirmed_hypotheses:
                f.write(f"- {h}\n")
            f.write("\n## âŒ Rejected Hypotheses\n")
            for h in research_report.rejected_hypotheses:
                f.write(f"- {h}\n")
            f.write("\n## ðŸ’¡ Key Insights\n")
            for h in research_report.key_insights:
                f.write(f"- {h}\n")
            f.write(f"\n## ðŸ”„ Pivot Recommendation\n{research_report.pivot_recommendation}\n")
            
        print(f"   -> Saved report to: {report_path}")
        
    except Exception as e:
        print(f"   -> Analyst Error: {e}")
        
    return {
        "research_report": research_report,
        "iteration_count": state["iteration_count"]
    }
