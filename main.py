import operator
from typing import Annotated, List, Optional, TypedDict, Union, Literal

from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langgraph.graph import END, StateGraph, START
from pydantic import BaseModel, Field
from dotenv import load_dotenv
import os
from config import llm_generator, llm_critic, llm_router, GENERATOR_SYSTEM_PROMPT, CRITIC_SYSTEM_PROMPT, RESEARCHER_SYSTEM_PROMPT, SIMULATION_SYSTEM_PROMPT, ANALYST_SYSTEM_PROMPT, MOCK_SIMULATION
import time
import json
import re
import pathlib

load_dotenv()

# --- 1. Data Structures (Pydantic Models) ---

class BusinessIdea(BaseModel):
    title: str = Field(description="ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚Ð°Ñ€Ñ‚Ð°Ð¿Ð° (Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼)")
    description: str = Field(description="Ð¡ÑƒÑ‚ÑŒ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°, Ñ†ÐµÐ½Ð½Ð¾ÑÑ‚Ð½Ð¾Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ (Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼)")
    monetization_strategy: str = Field(description="ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°: Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ°, ÐºÐ¾Ð¼Ð¸ÑÑÐ¸Ñ Ð¸ Ñ‚.Ð´. (Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼)")
    target_audience: str = Field(description="Ð¦ÐµÐ»ÐµÐ²Ð°Ñ Ð°ÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ Ð² Ð Ð¤ (Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼)")

class CritiqueFeedback(BaseModel):
    is_approved: bool = Field(description="True ÐµÑÐ»Ð¸ Ð¾Ñ†ÐµÐ½ÐºÐ° >= 8")
    feedback: str = Field(description="ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½Ð°Ñ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°, Ñ€Ð¸ÑÐºÐ¸ Ð¸ ÑÐ¾Ð²ÐµÑ‚Ñ‹ (Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ)")
    score: int = Field(description="ÐžÑ†ÐµÐ½ÐºÐ° Ð¾Ñ‚ 1 Ð´Ð¾ 10", ge=1, le=10)

# --- Synthetic CustDev Models ---

class Hypothesis(BaseModel):
    description: str = Field(description="Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ñ‹ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼")
    type: Literal["Problem", "Solution", "Monetization"] = Field(description="Ð¢Ð¸Ð¿ Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ñ‹")

class TargetPersona(BaseModel):
    role: str = Field(description="Ð Ð¾Ð»ÑŒ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: 'Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð±ÑƒÑ…Ð³Ð°Ð»Ñ‚ÐµÑ€' Ð¸Ð»Ð¸ 'ÐœÐ°Ð¼Ð° Ð² Ð´ÐµÐºÑ€ÐµÑ‚Ðµ')")
    archetype: str = Field(description="ÐŸÑÐ¸Ñ…Ð¾Ñ‚Ð¸Ð¿ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: 'ÐšÐ¾Ð½ÑÐµÑ€Ð²Ð°Ñ‚Ð¾Ñ€', 'ÐÐ¾Ð²Ð°Ñ‚Ð¾Ñ€', 'Ð¥ÐµÐ¹Ñ‚ÐµÑ€')")
    context: str = Field(description="ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¶Ð¸Ð·Ð½Ð¸/Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: 'Ð Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² 1Ð¡, Ð½ÐµÐ½Ð°Ð²Ð¸Ð´Ð¸Ñ‚ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ, Ð·Ð¿ 80Ðº')")
    name: str = Field(description="Ð ÑƒÑÑÐºÐ¾Ðµ Ð¸Ð¼Ñ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: 'Ð¢Ð°Ñ‚ÑŒÑÐ½Ð° Ð˜Ð²Ð°Ð½Ð¾Ð²Ð½Ð°')")

class InterviewGuide(BaseModel):
    target_personas: List[TargetPersona] = Field(description="Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¸Ð· 3-Ñ… ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… Ð»ÑŽÐ´ÐµÐ¹ Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð²ÑŒÑŽ")
    questions: List[str] = Field(description="Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð² ÑÑ‚Ð¸Ð»Ðµ 'The Mom Test'")
    hypotheses_to_test: List[Hypothesis] = Field(description="Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ñ‹ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸")

class UserPersona(BaseModel):
    name: str = Field(description="Ð˜Ð¼Ñ Ñ€ÐµÑÐ¿Ð¾Ð½Ð´ÐµÐ½Ñ‚Ð°")
    role: str = Field(description="Ð¡Ð¾Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð°Ñ Ñ€Ð¾Ð»ÑŒ / ÐŸÑ€Ð¾Ñ„ÐµÑÑÐ¸Ñ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: 'Ð¡ÐºÐµÐ¿Ñ‚Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð³Ð»Ð°Ð²Ð±ÑƒÑ…')")
    background: str = Field(description="ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ Ð±Ð¸Ð¾Ð³Ñ€Ð°Ñ„Ð¸Ñ Ð¸ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ¸ (ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð Ð¤)")
    
class InterviewResult(BaseModel):
    persona: UserPersona
    transcript_summary: str = Field(description="ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ Ð²Ñ‹Ð¶Ð¸Ð¼ÐºÐ° Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° (ÑÐ°Ð¼Ñ‹Ðµ Ð²Ð°Ð¶Ð½Ñ‹Ðµ Ð¸Ð½ÑÐ°Ð¹Ñ‚Ñ‹)")
    pain_level: int = Field(description="ÐÐ°ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð±Ð¾Ð»Ð¸Ñ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ð¾Ñ‚ 1 Ð´Ð¾ 10", ge=1, le=10)
    willingness_to_pay: int = Field(description="Ð“Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ð»Ð°Ñ‚Ð¸Ñ‚ÑŒ Ð¾Ñ‚ 1 Ð´Ð¾ 10", ge=1, le=10)
    
class ResearchReport(BaseModel):
    key_insights: List[str] = Field(description="Ð“Ð»Ð°Ð²Ð½Ñ‹Ðµ Ð¸Ð½ÑÐ°Ð¹Ñ‚Ñ‹ Ð¿Ð¾ÑÐ»Ðµ Ð²ÑÐµÑ… Ð¸Ð½Ñ‚ÐµÑ€Ð²ÑŒÑŽ")
    confirmed_hypotheses: List[str] = Field(description="Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·")
    rejected_hypotheses: List[str] = Field(description="Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¾Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð³Ð½ÑƒÑ‚Ñ‹Ñ… Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·")
    pivot_recommendation: str = Field(description="Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ ÐŸÑ€Ð¾Ð´ÑƒÐºÑ‚Ð°: Ñ‡Ñ‚Ð¾ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð² Ð¸Ð´ÐµÐµ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼)")

# --- 2. Graph State ---

class GraphState(TypedDict):
    initial_input: str
    current_idea: Optional[BusinessIdea]
    
    # --- Research Fields (Synthetic CustDev) ---
    interview_guide: Optional[InterviewGuide]  # Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð¸ Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ñ‹
    raw_interviews: List[InterviewResult]      # Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ "Ð·Ð²Ð¾Ð½ÐºÐ°"
    research_report: Optional[ResearchReport]  # Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð·
    # -------------------------------------------
    
    critique: Optional[CritiqueFeedback]
    iteration_count: int
    messages: List[BaseMessage]  # Optional, but good for history
    max_iterations: int  # Add configurable max iterations
    mode: Literal["full", "research_only"] # New field for mode selection
    
    # --- Configuration Flags ---
    enable_simulation: bool
    enable_critic: bool

# --- 3. Nodes & Logic ---

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ-"Ñ‡Ð¸ÑÑ‚Ð¸Ð»ÑŒÑ‰Ð¸Ðº", ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð²Ñ‹Ñ‚Ð°ÑÐºÐ¸Ð²Ð°ÐµÑ‚ JSON Ð¸Ð· Ð»ÑŽÐ±Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
def extract_json_from_text(text: str):
    try:
        # 1. ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð½Ð°Ð¹Ñ‚Ð¸ JSON Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð±Ð»Ð¾Ð³Ð° ÐºÐ¾Ð´Ð° ```json ... ```
        match = re.search(r"```json\n(.*?)\n```", text, re.DOTALL)
        if match:
            return match.group(1)
        
        # 2. ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð½Ð°Ð¹Ñ‚Ð¸ JSON Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ñ… ÑÐºÐ¾Ð±Ð¾Ðº { ... }
        match = re.search(r"(\{.*\})", text, re.DOTALL)
        if match:
            return match.group(1)
            
        # 3. Ð•ÑÐ»Ð¸ Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð½Ð°ÑˆÐ»Ð¸, Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ (Ð° Ð²Ð´Ñ€ÑƒÐ³ Ñ‚Ð°Ð¼ Ñ‡Ð¸ÑÑ‚Ñ‹Ð¹ JSON)
        return text
    except Exception:
        return text

def generator_node(state: GraphState) -> GraphState:
    print(f"\n--- GENERATOR NODE (Iteration {state['iteration_count']}) ---")
    
    # Ð’ÐÐ–ÐÐž: ÐœÑ‹ ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ .with_structured_output, Ñ‚Ð°Ðº ÐºÐ°Ðº Ð¾Ð½ Ð½ÐµÑÑ‚Ð°Ð±Ð¸Ð»ÐµÐ½
    # ÐœÑ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ invoke Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð¼ Ñ€ÑƒÐºÐ°Ð¼Ð¸.
    
    # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ JSON-ÑÑ…ÐµÐ¼Ñƒ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð½Ð°Ð»Ð° Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
    schema_instruction = """
    ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: Ð¢Ð²Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¡Ð¢Ð ÐžÐ“Ðž Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¼ JSON Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð¼.
    ÐÐ•Ð¢ Markdown Ð±Ð»Ð¾ÐºÐ¾Ð² ```json
    ÐÐ•Ð¢ Ð²Ð²Ð¾Ð´Ð½Ñ‹Ñ… ÑÐ»Ð¾Ð²
    ÐÐ•Ð¢ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ñ… Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ñ‚Ð¸Ð¿Ð° {"BusinessIdea": {...}}
    
    Ð¢ÐžÐ§ÐÐ«Ð™ Ð¤ÐžÐ ÐœÐÐ¢ (ÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¾Ð´Ð¸Ð½-Ð²-Ð¾Ð´Ð¸Ð½, Ð¼ÐµÐ½ÑÑ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð—ÐÐÐ§Ð•ÐÐ˜Ð¯):
    {
      "title": "ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚Ð°Ñ€Ñ‚Ð°Ð¿Ð° Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼",
      "description": "ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð° Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼",
      "monetization_strategy": "ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼",
      "target_audience": "Ð¦ÐµÐ»ÐµÐ²Ð°Ñ Ð°ÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼"
    }
    
    ÐŸÐ Ð˜ÐœÐ•Ð  ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐžÐ“Ðž ÐžÐ¢Ð’Ð•Ð¢Ð:
    {
      "title": "Ð—Ð°Ð±Ð¾Ñ‚Ð°.Ð Ð¤",
      "description": "ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° Ð´Ð»Ñ ÑƒÑ…Ð¾Ð´Ð° Ð·Ð° Ð¿Ð¾Ð¶Ð¸Ð»Ñ‹Ð¼Ð¸ Ð»ÑŽÐ´ÑŒÐ¼Ð¸",
      "monetization_strategy": "ÐŸÐ¾Ð´Ð¿Ð¸ÑÐºÐ° 3000â‚½/Ð¼ÐµÑ + ÐºÐ¾Ð¼Ð¸ÑÑÐ¸Ñ 15% Ð·Ð° ÑƒÑÐ»ÑƒÐ³Ð¸",
      "target_audience": "Ð Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‰Ð¸Ðµ Ð²Ð·Ñ€Ð¾ÑÐ»Ñ‹Ðµ Ð´ÐµÑ‚Ð¸ (35-55 Ð»ÐµÑ‚) Ð² Ð³Ð¾Ñ€Ð¾Ð´Ð°Ñ…-Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¸ÐºÐ°Ñ…"
    }
    
    Ð—ÐÐŸÐ Ð•Ð©Ð•ÐÐž:
    - Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€ÑƒÑÑÐºÐ¸Ðµ Ð¸Ð¼ÐµÐ½Ð° ÐºÐ»ÑŽÑ‡ÐµÐ¹ (ÐÐ°Ð·Ð²Ð°Ð½Ð¸ÐµÐŸÑ€Ð¾Ð´ÑƒÐºÑ‚Ð°, ÐšÑ€Ð°Ñ‚ÐºÐ¾ÐµÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ)
    - Ð¡Ð¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹
    - Ð”Ð¾Ð±Ð°Ð²Ð»ÑÑ‚ÑŒ ```json Ð¸Ð»Ð¸ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ markdown
    
    ÐÐÐ§Ð˜ÐÐÐ™ ÐžÐ¢Ð’Ð•Ð¢ Ð¡Ð ÐÐ—Ð£ Ð¡ { Ð¸ Ð—ÐÐšÐÐÐ§Ð˜Ð’ÐÐ™ }
    """
    
    if state["iteration_count"] == 0:
        print(">> Generating initial ZERO-TO-ONE concept...")
        user_content = f"""
        USER INPUT: '{state['initial_input']}'
        
        Task: Synthesize a Unicorn startup concept for the RUSSIAN MARKET (2025).
        """
    elif state.get("research_report") and not state.get("critique"):
        print(">> PIVOTING based on USER RESEARCH...")
        current_json = state["current_idea"].model_dump_json(indent=2)
        report_json = state["research_report"].model_dump_json(indent=2)
        
        user_content = f"""
        USER RESEARCH COMPLETED. UPDATE THE IDEA.
        
        PREVIOUS IDEA:
        {current_json}
        
        RESEARCH FINDINGS:
        {report_json}
        
        INSTRUCTIONS:
        1. Discard features that users rejected (see 'rejected_hypotheses').
        2. Double down on 'confirmed_hypotheses'.
        3. Implement the 'pivot_recommendation'.
        
        {schema_instruction}
        """
    elif state.get("critique"):
        print(">> PIVOTING based on Critique...")
        current_json = state["current_idea"].model_dump_json(indent=2)
        critique_json = state["critique"].model_dump_json(indent=2)
        
        user_content = f"""
        CRITIQUE RECEIVED. YOU MUST ITERATE OR PIVOT.
        
        PREVIOUS IDEA:
        {current_json}
        
        CRITIC FEEDBACK:
        {critique_json}
        
        INSTRUCTIONS:
        1. Address the fatal flaws.
        2. Focus on Russian local tech (VK, Telegram, SPB, Gosuslugi).
        
        {schema_instruction}
        """
    else:
        # Fallback - should not happen in normal flow, but prevents crashes
        print(">> WARNING: Unexpected state, generating fallback...")
        user_content = f"""
        USER INPUT: '{state['initial_input']}'
        
        Task: Refine the existing idea.
        {schema_instruction}
        """

    messages = [
        SystemMessage(content=GENERATOR_SYSTEM_PROMPT),
        HumanMessage(content=user_content)
    ]

    # --- RETRY & PARSE LOGIC ---
    new_idea = None
    last_error = None
    
    for attempt in range(3):
        try:
            print(f"   -> Invoking LLM (Attempt {attempt + 1})...")
            
            response = llm_generator.invoke(messages)
            raw_content = response.content
            
            # --- FIX: ÐžÐ‘Ð ÐÐ‘ÐžÐ¢ÐšÐ Ð¡ÐŸÐ˜Ð¡ÐšÐ ---
            # Ð•ÑÐ»Ð¸ Gemini Ð²ÐµÑ€Ð½ÑƒÐ» ÑÐ¿Ð¸ÑÐ¾Ðº Ð±Ð»Ð¾ÐºÐ¾Ð² [{'type': 'text', 'text': '...'}]
            if isinstance(raw_content, list):
                print(f"   -> Detected list output, converting to string...")
                raw_content = "".join([
                    block.get("text", "") 
                    for block in raw_content 
                    if isinstance(block, dict) and "text" in block
                ])
            # -----------------------------
            
            # Ð§Ð¸ÑÑ‚Ð¸Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚
            cleaned_json_str = extract_json_from_text(raw_content)
            
            # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð² Python dict, Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ð² Pydantic
            data_dict = json.loads(cleaned_json_str)
            new_idea = BusinessIdea(**data_dict)
            
            break # Ð£ÑÐ¿ÐµÑ…!
            
        except Exception as e:
            print(f"   -> Parse Error on attempt {attempt + 1}: {e}")
            print(f"   -> Raw Output causing error: {raw_content[:500]}...") 
            last_error = e

    if new_idea is None:
        print("   -> CRITICAL ERROR: Failed to parse JSON from Generator.")
        # Ð§Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ ÐºÑ€Ð°ÑˆÐ¸Ñ‚ÑŒ Ð²ÐµÑÑŒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ, Ð²ÐµÑ€Ð½ÐµÐ¼ Ð·Ð°Ð³Ð»ÑƒÑˆÐºÑƒ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¾Ð¹
        new_idea = BusinessIdea(
            title="ÐžÑˆÐ¸Ð±ÐºÐ° Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ (Parsing)",
            description=f"ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð²ÐµÑ€Ð½ÑƒÐ»Ð° Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON. ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {str(last_error)}",
            monetization_strategy="N/A",
            target_audience="N/A"
        )

    print(f"   -> Generated: {new_idea.title}")

    # If we are iterating (critique exists), we should clear the previous research report and critique
    # to allow for a fresh research cycle if enabled.
    return {
        "current_idea": new_idea,
        "iteration_count": state["iteration_count"] + 1,
        "research_report": None, # Clear for next cycle
        "critique": None         # Clear for next cycle
    }

def critic_node(state: GraphState) -> GraphState:
    """
    Simulates and critiques the idea using ChatGPT 5.1 (Reasoning).
    """
    print("\n--- CRITIC NODE ---")
    
    # 1. Bind Structured Output
    structured_llm = llm_critic.with_structured_output(CritiqueFeedback)
    
    current_idea = state["current_idea"]
    
    # 2. Construct Data-Only User Message
    # The System Prompt already tells GPT-5.1 to "Simulate", so we just provide the data.
    user_content = f"""
    CANDIDATE STARTUP IDEA FOR EVALUATION:
    
    {current_idea.model_dump_json(indent=2)}
    
    Run your simulation and provide the verdict.
    """
    
    # 3. Invoke LLM with the Reasoning System Prompt
    messages = [
        SystemMessage(content=CRITIC_SYSTEM_PROMPT),
        HumanMessage(content=user_content)
    ]
    
    try:
        feedback = structured_llm.invoke(messages)
        
        # Validate the response
        if feedback is None:
            raise ValueError("LLM returned None. Check API key and model availability.")
        
        print(f"   -> Verdict: {feedback.is_approved} (Score: {feedback.score}/10)")
        print(f"   -> Key Feedback: {feedback.feedback[:100]}...") # Print preview
    except Exception as e:
        print(f"   -> ERROR in critic_node: {e}")
        # Return a default critique to prevent crash
        feedback = CritiqueFeedback(
            is_approved=False,
            feedback=f"Critique failed due to LLM error: {str(e)}",
            score=1
        )
        
    return {"critique": feedback}

def researcher_node(state: GraphState) -> GraphState:
    """
    Generates hypotheses and an interview guide using Gemini 3 Pro.
    Saves the guide to a local file.
    """
    print(f"\n--- RESEARCHER NODE ---")
    
    current_idea = state["current_idea"]
    
    # 1. Construct User Message
    schema_instruction = """
    ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž Ð”Ð›Ð¯ JSON:
    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ðµ Ð¸Ð¼ÐµÐ½Ð° Ð¿Ð¾Ð»ÐµÐ¹: target_personas, questions, hypotheses_to_test
    
    ÐŸÐ Ð˜ÐœÐ•Ð  ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐžÐ“Ðž ÐžÐ¢Ð’Ð•Ð¢Ð:
    {
      "target_personas": [
        {
          "name": "Ð¢Ð°Ñ‚ÑŒÑÐ½Ð° Ð˜Ð²Ð°Ð½Ð¾Ð²Ð½Ð°",
          "role": "Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð±ÑƒÑ…Ð³Ð°Ð»Ñ‚ÐµÑ€",
          "archetype": "ÐšÐ¾Ð½ÑÐµÑ€Ð²Ð°Ñ‚Ð¾Ñ€",
          "context": "Ð Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² 1Ð¡ 15 Ð»ÐµÑ‚, Ð½ÐµÐ½Ð°Ð²Ð¸Ð´Ð¸Ñ‚ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ, Ð·Ð¿ 80Ðº"
        },
        {
          "name": "ÐÐ»ÐµÐºÑÐµÐ¹",
          "role": "ÐŸÑ€Ð¾Ð´Ð°ÐºÑ‚-Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€",
          "archetype": "ÐÐ¾Ð²Ð°Ñ‚Ð¾Ñ€",
          "context": "Ð›ÑŽÐ±Ð¸Ñ‚ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ, Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² ÑÑ‚Ð°Ñ€Ñ‚Ð°Ð¿Ðµ"
        },
        {
          "name": "Ð•Ð»ÐµÐ½Ð°",
          "role": "Ð ÑƒÐºÐ¾Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ Ð¾Ñ‚Ð´ÐµÐ»Ð°",
          "archetype": "Ð¥ÐµÐ¹Ñ‚ÐµÑ€",
          "context": "ÐÐµ Ð´Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¼ Ñ€ÐµÑˆÐµÐ½Ð¸ÑÐ¼, Ð¸Ñ‰ÐµÑ‚ Ð¿Ð¾Ð´Ð²Ð¾Ñ…"
        }
      ],
      "questions": ["Ð’Ð¾Ð¿Ñ€Ð¾Ñ 1", "Ð’Ð¾Ð¿Ñ€Ð¾Ñ 2"],
      "hypotheses_to_test": [
        {"description": "Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 1", "type": "Problem"},
        {"description": "Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 2", "type": "Solution"},
        {"description": "Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 3", "type": "Monetization"}
      ]
    }
    
    Ð’ÐÐ–ÐÐž: type Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¢ÐžÐ›Ð¬ÐšÐž "Problem", "Solution" Ð¸Ð»Ð¸ "Monetization" (ÐÐ• "Willingness to Pay"!)
    target_personas Ð´Ð¾Ð»Ð¶ÐµÐ½ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ÑŒ Ð¢ÐžÐ§ÐÐž 3 Ð¾Ð±ÑŠÐµÐºÑ‚Ð° Ñ Ð¿Ð¾Ð»ÑÐ¼Ð¸: role, archetype, context, name (Ð²ÑÐµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼)
    ÐÐµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ€ÑƒÑÑÐºÐ¸Ðµ ÐºÐ»ÑŽÑ‡Ð¸ Ð² JSON
    """
    
    user_content = f"""
    ANALYZE THIS BUSINESS IDEA AND PREPARE USER RESEARCH:
    
    {current_idea.model_dump_json(indent=2)}
    
    Task: Create a 'Mom Test' interview guide to validate this idea in the Russian market.
    
    {schema_instruction}
    """
    
    messages = [
        SystemMessage(content=RESEARCHER_SYSTEM_PROMPT),
        HumanMessage(content=user_content)
    ]
    
    # 2. Invoke LLM
    # We use the same manual parsing logic as generator_node for stability with Gemini
    interview_guide = None
    last_error = None
    
    for attempt in range(3):
        try:
            print(f"   -> Invoking Researcher (Attempt {attempt + 1})...")
            response = llm_generator.invoke(messages)
            raw_content = response.content
            
            if isinstance(raw_content, list):
                raw_content = "".join([b.get("text", "") for b in raw_content if isinstance(b, dict)])
                
            cleaned_json_str = extract_json_from_text(raw_content)
            data_dict = json.loads(cleaned_json_str)
            interview_guide = InterviewGuide(**data_dict)
            break
        except Exception as e:
            print(f"   -> Researcher Parse Error: {e}")
            last_error = e
            
    if interview_guide is None:
        print("   -> CRITICAL: Researcher failed to generate guide.")
        # Return empty/default to avoid crash
        return {"interview_guide": None, "iteration_count": state["iteration_count"]}

    print(f"   -> Generated Guide with {len(interview_guide.target_personas)} personas and {len(interview_guide.hypotheses_to_test)} hypotheses")

    # 3. File Persistence
    try:
        # Sanitize title for folder name
        safe_title = re.sub(r'[<>:"/\\|?*]', '', current_idea.title).strip().replace(' ', '_')
        # Limit length just in case
        safe_title = safe_title[:50]
        
        base_dir = pathlib.Path("experiments")
        experiment_dir = base_dir / safe_title
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        file_path = experiment_dir / "interview_guide.md"
        
        # Format Markdown with new structure
        md_content = f"# Interview Guide: {current_idea.title}\n\n"
        
        md_content += "## Target Personas\n\n"
        for i, persona in enumerate(interview_guide.target_personas, 1):
            md_content += f"### Persona {i}: {persona.name}\n"
            md_content += f"- **Role:** {persona.role}\n"
            md_content += f"- **Archetype:** {persona.archetype}\n"
            md_content += f"- **Context:** {persona.context}\n\n"
        
        md_content += "## Hypotheses to Test\n"
        for h in interview_guide.hypotheses_to_test:
            md_content += f"- **[{h.type}]** {h.description}\n"
            
        md_content += "\n## Questions (The Mom Test)\n"
        for i, q in enumerate(interview_guide.questions, 1):
            md_content += f"{i}. {q}\n"
            
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(md_content)
            
        print(f"   -> Saved guide to: {file_path}")
        
    except Exception as e:
        print(f"   -> File Save Error: {e}")

    return {
        "interview_guide": interview_guide,
        "iteration_count": state["iteration_count"]
    }

def simulation_node(state: GraphState) -> GraphState:
    """
    Simulates 3 distinct user interviews based on the guide.
    """
    print(f"\n--- SIMULATION NODE ---")
    
    interview_guide = state.get("interview_guide")
    if not interview_guide:
        print("   -> CRITICAL: No interview guide found.")
        return state
        
    current_idea = state["current_idea"]
    
    # Get personas from interview guide (generated by researcher)
    personas = interview_guide.target_personas
    if not personas or len(personas) == 0:
        print("   -> CRITICAL: No personas in interview guide.")
        return state
    
    print(f"   -> Found {len(personas)} personas to interview")
    
    raw_interviews = []
    
    # Prepare File for Transcript
    try:
        safe_title = re.sub(r'[<>:"/\\|?*]', '', current_idea.title).strip().replace(' ', '_')[:50]
        base_dir = pathlib.Path("experiments")
        experiment_dir = base_dir / safe_title
        experiment_dir.mkdir(parents=True, exist_ok=True)
        transcript_path = experiment_dir / "interviews_transcript.md"
        
        # Clear previous file
        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(f"# User Interviews: {current_idea.title}\n\n")
            
    except Exception as e:
        print(f"   -> File Setup Error: {e}")
        transcript_path = None

    # Loop & Simulate
    for p in personas:
        print(f"   -> Simulating interview with {p.name}...")
        
        # MOCK MODE CHECK
        if MOCK_SIMULATION:
            print(f"   -> [MOCK MODE] Skipping real LLM call for {p.name}")
            result = InterviewResult(
                persona=UserPersona(name=p.name, role=p.role, background=f"{p.archetype}: {p.context}"),
                transcript_summary=f"ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ {p.name} Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚, Ñ‡Ñ‚Ð¾ Ð¸Ð´ÐµÑ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð°Ñ, Ð½Ð¾ Ð´Ð¾Ñ€Ð¾Ð³Ð¾. Ð¥Ð¾Ñ‡ÐµÑ‚ Telegram-Ð±Ð¾Ñ‚ Ð²Ð¼ÐµÑÑ‚Ð¾ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ.",
                pain_level=7,
                willingness_to_pay=4
            )
            raw_interviews.append(result)
            
            # Still write to file
            if transcript_path:
                with open(transcript_path, "a", encoding="utf-8") as f:
                    f.write(f"## Interview with {result.persona.name}\n")
                    f.write(f"**Role:** {result.persona.role}\n")
                    f.write(f"**Pain Level:** {result.pain_level}/10\n")
                    f.write(f"**Willingness to Pay:** {result.willingness_to_pay}/10\n\n")
                    f.write(f"### Transcript Summary\n{result.transcript_summary}\n\n")
                    f.write("---\n\n")
            continue
        
        # REAL LLM MODE
        user_content = f"""
        Ð¢Ð« Ð˜Ð“Ð ÐÐ•Ð¨Ð¬ Ð ÐžÐ›Ð¬:
        Ð˜Ð¼Ñ: {p.name}
        ÐŸÑ€Ð¾Ñ„ÐµÑÑÐ¸Ñ: {p.role}
        ÐŸÑÐ¸Ñ…Ð¾Ñ‚Ð¸Ð¿: {p.archetype}
        ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚: {p.context}
        
        Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° â€” Ð¿Ñ€Ð¾Ð¹Ñ‚Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð²ÑŒÑŽ Ð¿Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñƒ.
        Ð‘ÑƒÐ´ÑŒ {p.archetype}. ÐžÐ¿Ð¸Ñ€Ð°Ð¹ÑÑ Ð½Ð° ÑÐ²Ð¾Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚: {p.context}
        ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ñ‡ÐµÑÑ‚Ð½Ð¾. Ð•ÑÐ»Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ñ‚ÐµÐ±Ðµ Ð½Ðµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚ â€” ÑÐºÐ°Ð¶Ð¸ Ð¾Ð± ÑÑ‚Ð¾Ð¼ Ð¿Ñ€ÑÐ¼Ð¾.
        ÐÐµ Ð¿Ñ‹Ñ‚Ð°Ð¹ÑÑ ÑƒÐ³Ð¾Ð´Ð¸Ñ‚ÑŒ Ð¸Ð½Ñ‚ÐµÑ€Ð²ÑŒÑŽÐµÑ€Ñƒ.
        
        Ð’ÐžÐŸÐ ÐžÐ¡Ð« Ð˜ÐÐ¢Ð•Ð Ð’Ð¬Ð®Ð•Ð Ð:
        {json.dumps(interview_guide.questions, ensure_ascii=False, indent=2)}
        
        Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¸ meta-analysis JSON.
        """
        
        messages = [
            SystemMessage(content=SIMULATION_SYSTEM_PROMPT),
            HumanMessage(content=user_content)
        ]
        
        # Invoke LLM (GPT-5.1 / Critic Model)
        try:
            response = llm_critic.invoke(messages)
            raw_content = response.content
            
            cleaned_json_str = extract_json_from_text(raw_content)
            data_dict = json.loads(cleaned_json_str)
            
            # Inject Name/Role into the result manually since LLM might generate random ones
            if "persona" not in data_dict:
                data_dict["persona"] = {}
            data_dict["persona"]["name"] = p.name
            data_dict["persona"]["role"] = p.role
            if "background" not in data_dict["persona"]:
                 data_dict["persona"]["background"] = f"{p.archetype}: {p.context}"

            result = InterviewResult(**data_dict)
            raw_interviews.append(result)
            
            # Append to Transcript File
            if transcript_path:
                with open(transcript_path, "a", encoding="utf-8") as f:
                    f.write(f"## Interview with {result.persona.name}\n")
                    f.write(f"**Role:** {result.persona.role}\n")
                    f.write(f"**Pain Level:** {result.pain_level}/10\n")
                    f.write(f"**Willingness to Pay:** {result.willingness_to_pay}\n\n")
                    f.write(f"### Transcript Summary\n{result.transcript_summary}\n\n")
                    # Note: We don't have the full raw dialogue text in the JSON model currently, 
                    # only the summary. The prompt asks for "Transcript" in the text generation 
                    # but the JSON schema only captures summary. 
                    # Ideally, we should capture the full text too, but for now we follow the schema.
                    f.write("---\n\n")
                    
        except Exception as e:
            print(f"   -> Simulation Error for {p['name']}: {e}")
            
    return {
        "raw_interviews": raw_interviews,
        "iteration_count": state["iteration_count"]
    }

def analyst_node(state: GraphState) -> GraphState:
    """
    Analyzes interview transcripts and generates a research report.
    """
    print(f"\n--- ANALYST NODE ---")
    
    raw_interviews = state.get("raw_interviews", [])
    if not raw_interviews:
        print("   -> CRITICAL: No interviews found.")
        return state
        
    current_idea = state["current_idea"]
    
    # 1. Aggregate Context
    transcripts_text = ""
    for i, interview in enumerate(raw_interviews, 1):
        transcripts_text += f"INTERVIEW {i} ({interview.persona.role}):\n"
        transcripts_text += f"Summary: {interview.transcript_summary}\n"
        transcripts_text += f"Pain Level: {interview.pain_level}/10\n"
        transcripts_text += f"Willingness to Pay: {interview.willingness_to_pay}/10\n\n"
        
    user_content = f"""
    ANALYZE THESE INTERVIEWS:
    
    {transcripts_text}
    
    Task: Validate hypotheses and recommend a pivot.
    
    ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: Ð¢Ð²Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¡Ð¢Ð ÐžÐ“Ðž Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¼ JSON Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð¼.
    ÐÐ•Ð¢ Markdown Ð±Ð»Ð¾ÐºÐ¾Ð² ```json
    ÐÐ•Ð¢ Ð²Ð²Ð¾Ð´Ð½Ñ‹Ñ… ÑÐ»Ð¾Ð²
    
    Ð¢ÐžÐ§ÐÐ«Ð™ Ð¤ÐžÐ ÐœÐÐ¢ (ÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¾Ð´Ð¸Ð½-Ð²-Ð¾Ð´Ð¸Ð½):
    {{
      "key_insights": ["Ð˜Ð½ÑÐ°Ð¹Ñ‚ 1 Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹", "Ð˜Ð½ÑÐ°Ð¹Ñ‚ 2 Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹"],
      "confirmed_hypotheses": ["Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 1", "Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 2"],
      "rejected_hypotheses": ["Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ð° 3"],
      "pivot_recommendation": "Ð§ÐµÑ‚ÐºÐ°Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹"
    }}
    
    Ð—ÐÐŸÐ Ð•Ð©Ð•ÐÐž:
    - Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹ Ñ‚Ð¸Ð¿Ð° {{"insight": "Ñ‚ÐµÐºÑÑ‚"}}
    - key_insights Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¼Ð°ÑÑÐ¸Ð²Ð¾Ð¼ Ð¡Ð¢Ð ÐžÐš, Ð½Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð²
    - Ð’ÑÐµ Ð¿Ð¾Ð»Ñ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹
    
    ÐÐÐ§Ð˜ÐÐÐ™ ÐžÐ¢Ð’Ð•Ð¢ Ð¡Ð ÐÐ—Ð£ Ð¡ {{ Ð¸ Ð—ÐÐšÐÐÐ§Ð˜Ð’ÐÐ™ }}
    """
    
    messages = [
        SystemMessage(content=ANALYST_SYSTEM_PROMPT),
        HumanMessage(content=user_content)
    ]
    
    # 2. Invoke LLM (Gemini 3 Pro)
    research_report = None
    try:
        response = llm_generator.invoke(messages)
        raw_content = response.content
        
        if isinstance(raw_content, list):
            raw_content = "".join([b.get("text", "") for b in raw_content if isinstance(b, dict)])
            
        cleaned_json_str = extract_json_from_text(raw_content)
        data_dict = json.loads(cleaned_json_str)
        research_report = ResearchReport(**data_dict)
        
        print(f"   -> Pivot Recommendation: {research_report.pivot_recommendation[:100]}...")
        
        # 3. File Persistence
        safe_title = re.sub(r'[<>:"/\\|?*]', '', current_idea.title).strip().replace(' ', '_')[:50]
        base_dir = pathlib.Path("experiments")
        experiment_dir = base_dir / safe_title
        experiment_dir.mkdir(parents=True, exist_ok=True)
        report_path = experiment_dir / "research_report.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"# Research Report: {current_idea.title}\n\n")
            f.write("## âœ… Confirmed Hypotheses\n")
            for h in research_report.confirmed_hypotheses:
                f.write(f"- {h}\n")
            f.write("\n## âŒ Rejected Hypotheses\n")
            for h in research_report.rejected_hypotheses:
                f.write(f"- {h}\n")
            f.write("\n## ðŸ’¡ Key Insights\n")
            for h in research_report.key_insights:
                f.write(f"- {h}\n")
            f.write(f"\n## ðŸ”„ Pivot Recommendation\n{research_report.pivot_recommendation}\n")
            
        print(f"   -> Saved report to: {report_path}")
        
    except Exception as e:
        print(f"   -> Analyst Error: {e}")
        
    return {
        "research_report": research_report,
        "iteration_count": state["iteration_count"]
    }

# --- 4. Workflow (The Graph) ---

def route_after_generator(state: GraphState) -> str:
    """
    Determines where to go after Generator.
    Logic:
    1. If Simulation Enabled AND No Research Report -> Researcher
    2. If Critic Enabled -> Critic
    3. Else -> END
    """
    research_report = state.get("research_report")
    enable_simulation = state.get("enable_simulation", True) # Default True for backward compatibility
    enable_critic = state.get("enable_critic", True)         # Default True
    
    # If Simulation is enabled, we MUST do research first (unless already done for this cycle)
    # Note: generator_node clears research_report on new iteration, so this works for loops too.
    if enable_simulation and research_report is None:
        return "researcher"
        
    # If we have research (or sim disabled), check if we want critique
    if enable_critic:
        return "critic"
        
    # If neither (Generation Only), stop
    return "end"

def route_after_analyst(state: GraphState) -> str:
    """
    After analyst, always go back to generator to apply research insights.
    """
    return "generator"

def should_continue(state: GraphState) -> str:
    """
    Determines the next step in the graph.
    """
    critique = state.get("critique")
    iteration = state["iteration_count"]
    max_iterations = state.get("max_iterations", 5)  # Default 5 if not specified
    
    if critique and critique.is_approved:
        return "end"
    
    if iteration >= max_iterations:
        print(f"Max iterations reached ({max_iterations}).")
        return "end"
    
    return "continue"

# Build the graph
workflow = StateGraph(GraphState)

workflow.add_node("generator", generator_node)
workflow.add_node("researcher", researcher_node)
workflow.add_node("simulation", simulation_node)
workflow.add_node("analyst", analyst_node)
workflow.add_node("critic", critic_node)

workflow.add_edge(START, "generator")

# Conditional edge after generator
workflow.add_conditional_edges(
    "generator",
    route_after_generator,
    {
        "researcher": "researcher",
        "critic": "critic",
        "end": END
    }
)

workflow.add_edge("researcher", "simulation")
workflow.add_edge("simulation", "analyst")
workflow.add_edge("analyst", "generator") # Loop back for Pivot

workflow.add_conditional_edges(
    "critic",
    should_continue,
    {
        "continue": "generator",
        "end": END
    }
)

# Compile the Full Graph
app = workflow.compile()

# --- 5. Main Execution ---

if __name__ == "__main__":
    print("Starting Iterative Idea Validator...")
    
    initial_input = "Uber for walking cats"
    
    initial_state = GraphState(
        initial_input=initial_input,
        current_idea=None,
        critique=None,
        iteration_count=0,
        messages=[]
    )
    
    # Run the graph
    # stream() yields the state updates as they happen
    for output in app.stream(initial_state):
        pass # We are printing inside the nodes for this demo
        
    print("\n--- FINAL STATE ---")
    # Since stream yields updates, we might want to capture the final result differently 
    # or just rely on the prints for this skeleton. 
    # To get the final state object, we can use invoke() instead of stream() if we want the return value.
    
    final_state = app.invoke(initial_state)
    
    if final_state.get("current_idea"):
        print(f"Final Idea Title: {final_state['current_idea'].title}")
        print(f"Final Idea Description: {final_state['current_idea'].description}")
    
    if final_state.get("critique"):
        print(f"Final Score: {final_state['critique'].score}")
        print(f"Approved: {final_state['critique'].is_approved}")
